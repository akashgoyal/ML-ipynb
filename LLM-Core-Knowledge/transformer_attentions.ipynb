{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hlV0szziZmnH"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8MtLGnm2bm7R"
      },
      "source": [
        "<figure>\n",
        "  <figcaption>Transformer Tokens Attention</figcaption>\n",
        "  <img src=\"https://drive.google.com/uc?export=view&id=1zg2ZJ8Vnuxth41gGeGg41NydFyDr8TSq\" alt=\"Sample Image\" width=\"500\">\n",
        "</figure>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-RXFGuGUcpzN"
      },
      "source": [
        "<figure>\n",
        "  <figcaption>Attention Maths</figcaption>\n",
        "  <img src=\"https://drive.google.com/uc?export=view&id=1TVgwT5BTpI_5hZBdm0RxaaLz_Qwgdlb6\" alt=\"Sample Image\" width=\"250\">\n",
        "</figure>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rFbnDOg_Zevv",
        "outputId": "d164771f-e6e2-4426-e6e8-ebd48c5c0b70"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Input:\n",
            " tensor([[[ 0.9393,  1.1324,  0.1013,  1.4069],\n",
            "         [-0.7529,  1.5224, -1.0648, -0.5474],\n",
            "         [ 0.1634, -0.8008, -0.3369, -0.4246]],\n",
            "\n",
            "        [[-0.5970, -1.1192, -1.0540,  0.4191],\n",
            "         [ 1.8294, -0.3668, -0.3958,  1.0895],\n",
            "         [-0.2905, -0.2080,  1.6845, -0.9910]]])\n",
            "Output:\n",
            " tensor([[[ 0.4623,  0.4141,  0.2415,  0.2815],\n",
            "         [ 0.4483,  0.3984,  0.2311,  0.2502],\n",
            "         [ 0.5254,  0.3633,  0.2413,  0.3072]],\n",
            "\n",
            "        [[ 0.3711,  0.0515,  0.1035, -0.1709],\n",
            "         [ 0.4296, -0.0672,  0.0026, -0.1290],\n",
            "         [ 0.3522, -0.1485, -0.2304, -0.1724]]], grad_fn=<UnsafeViewBackward0>)\n",
            "Attention Weights:\n",
            " tensor([[[0.3940, 0.3181, 0.2878],\n",
            "         [0.4007, 0.2846, 0.3147],\n",
            "         [0.3259, 0.3377, 0.3364]],\n",
            "\n",
            "        [[0.3175, 0.4376, 0.2449],\n",
            "         [0.3682, 0.3157, 0.3161],\n",
            "         [0.2334, 0.2921, 0.4746]]], grad_fn=<SoftmaxBackward0>)\n"
          ]
        }
      ],
      "source": [
        "class SimpleAttention(nn.Module):\n",
        "    def __init__(self, embed_size):\n",
        "        super(SimpleAttention, self).__init__()\n",
        "        self.embed_size = embed_size\n",
        "        self.query_linear = nn.Linear(embed_size, embed_size)\n",
        "        self.key_linear = nn.Linear(embed_size, embed_size)\n",
        "        self.value_linear = nn.Linear(embed_size, embed_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x (batch_size, seq_length, embed_size)\n",
        "        Q = self.query_linear(x)  # (bsz, seq, emb)\n",
        "        K = self.key_linear(x)    # (bsz, seq, emb)\n",
        "        V = self.value_linear(x)  # (bsz, seq, emb)\n",
        "\n",
        "        # Q.K / root(d)\n",
        "        attention_scores = torch.matmul(Q, K.transpose(-2, -1)) / (self.embed_size ** 0.5)  # (bsz, seq, seq)\n",
        "        # softmax ( Q.K/root(d) )\n",
        "        attention_weights = F.softmax(attention_scores, dim=-1)  # (bsz, seq, seq)\n",
        "        # softmax ( Q.K/root(d) ) . V\n",
        "        out = torch.matmul(attention_weights, V)  # (bsz, seq, emb)\n",
        "\n",
        "        return out, attention_weights\n",
        "\n",
        "# usage\n",
        "batch_size, seq_length, embed_size = 2, 3, 4\n",
        "x = torch.randn(batch_size, seq_length, embed_size)\n",
        "attention_layer = SimpleAttention(embed_size)\n",
        "out, attention_weights = attention_layer(x)\n",
        "\n",
        "print(\"Input:\", x)\n",
        "print(\"Output:\", out)\n",
        "print(\"Attention Weights:\", attention_weights)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6oMsV_3ybhYn"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WiolgaWDHJJ2"
      },
      "source": [
        "<figure>\n",
        "  <figcaption>Multi Head Attention</figcaption>\n",
        "  <img src=\"https://drive.google.com/uc?export=view&id=1rs5-ptxkx2ycBfll7eT3-eKclpb1MkH8\" alt=\"Sample Image\" width=\"500\">\n",
        "</figure>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f1dQ-3_1biAj"
      },
      "outputs": [],
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, embed_size, num_heads):\n",
        "        super(MultiHeadAttention, self).__init__()\n",
        "        assert embed_size % num_heads == 0, \"Embedding size must be divisible by number of heads\"\n",
        "\n",
        "        self.embed_size = embed_size\n",
        "        self.num_heads = num_heads\n",
        "        self.head_dim = embed_size // num_heads\n",
        "\n",
        "        self.query_linear = nn.Linear(embed_size, embed_size)\n",
        "        self.key_linear = nn.Linear(embed_size, embed_size)\n",
        "        self.value_linear = nn.Linear(embed_size, embed_size)\n",
        "        self.fc_out = nn.Linear(embed_size, embed_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        batch_size, seq_length, embed_size = x.size()\n",
        "\n",
        "        # Linear projections\n",
        "        Q = self.query_linear(x)  # (bsz, seq, emb)\n",
        "        K = self.key_linear(x)    # (bsz, seq, emb)\n",
        "        V = self.value_linear(x)  # (bsz, seq, emb)\n",
        "\n",
        "        # reshape - # (bsz, seq, num_heads, head_dim)\n",
        "        Q = Q.view(batch_size, seq_length, self.num_heads, self.head_dim)\n",
        "        K = K.view(batch_size, seq_length, self.num_heads, self.head_dim)\n",
        "        V = V.view(batch_size, seq_length, self.num_heads, self.head_dim)\n",
        "\n",
        "        # transpose - # (bsz, num_heads, seq, head_dim)\n",
        "        Q = Q.transpose(1, 2)\n",
        "        K = K.transpose(1, 2)\n",
        "        V = V.transpose(1, 2)\n",
        "\n",
        "        # Scaled dot-product attention\n",
        "        attention_scores = torch.matmul(Q, K.transpose(-2, -1)) / (self.head_dim ** 0.5)  # (bsz, num_heads, seq, seq)\n",
        "        attention_weights = F.softmax(attention_scores, dim=-1)  # (bsz, num_heads, seq, seq)\n",
        "        out = torch.matmul(attention_weights, V)  # (bsz, num_heads, seq, head_dim)\n",
        "\n",
        "        # concat heads\n",
        "        out = out.transpose(1, 2).contiguous().view(batch_size, seq_length, embed_size) # (bsz, seq, emb)\n",
        "        # final\n",
        "        out = self.fc_out(out)\n",
        "        return out, attention_weights\n",
        "\n",
        "# usage\n",
        "batch_size, seq_length, embed_size, num_heads = 2, 3, 4, 2\n",
        "x = torch.randn(batch_size, seq_length, embed_size)\n",
        "multi_head_attention_layer = MultiHeadAttention(embed_size, num_heads)\n",
        "out, attention_weights = multi_head_attention_layer(x)\n",
        "\n",
        "print(\"Input:\", x)\n",
        "print(\"Output:\", out)\n",
        "print(\"Attention Weights:\", attention_weights)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ipqn2zCoPdvd"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1H8raK_gPeEA"
      },
      "source": [
        "<figure>\n",
        "  <figcaption>Cross Attention</figcaption>\n",
        "  <img src=\"https://drive.google.com/uc?export=view&id=1lhGJrpdrNF2lu8URGcUdUomngPichK5i\" alt=\"Sample Image\" width=\"500\">\n",
        "</figure>\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R3cE5IbrQNtS"
      },
      "outputs": [],
      "source": [
        "class CrossAttention(nn.Module):\n",
        "    def __init__(self, embed_size):\n",
        "        super(CrossAttention, self).__init__()\n",
        "        self.embed_size = embed_size\n",
        "        self.query_linear = nn.Linear(embed_size, embed_size)\n",
        "        self.key_linear = nn.Linear(embed_size, embed_size)\n",
        "        self.value_linear = nn.Linear(embed_size, embed_size)\n",
        "\n",
        "    def forward(self, x, context):\n",
        "        Q = self.query_linear(x)         # (bsz, seq, emb)\n",
        "        K = self.key_linear(context)     # (bsz, context_seq, emb)\n",
        "        V = self.value_linear(context)   # (bsz, context_seq, emb)\n",
        "\n",
        "        # Q.K / root(d)\n",
        "        attention_scores = torch.matmul(Q, K.transpose(-2, -1)) / (self.embed_size ** 0.5)  # (bsz, seq, context_seq)\n",
        "        # softmax ( Q.K/root(d) )\n",
        "        attention_weights = F.softmax(attention_scores, dim=-1)  # (bsz, seq, context_seq)\n",
        "        # softmax ( Q.K/root(d) ) . V\n",
        "        out = torch.matmul(attention_weights, V)  # (bsz, seq, emb)\n",
        "        #\n",
        "        return out, attention_weights\n",
        "\n",
        "# usage\n",
        "batch_size, seq_length, context_length, embed_size = 2, 3, 5, 4\n",
        "x = torch.randn(batch_size, seq_length, embed_size)\n",
        "context = torch.randn(batch_size, context_length, embed_size)\n",
        "attention_layer = CrossAttention(embed_size)\n",
        "out, attention_weights = attention_layer(x, context)\n",
        "\n",
        "print(\"Input:\", x)\n",
        "print(\"Context:\", context)\n",
        "print(\"Output:\", out)\n",
        "print(\"Attention Weights:\", attention_weights)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hoORQ3UfW1xX"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kfFWwcCcSOwK"
      },
      "source": [
        "<figure>\n",
        "  <figcaption>Cross Attention</figcaption>\n",
        "  <img src=\"https://drive.google.com/uc?export=view&id=1g9l2FFRxJHAXwnpMSE8n05w3fqQNkySn\" alt=\"Grouped Query Attention Image\" width=\"500\">\n",
        "</figure>\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zr0zZppGSPPW"
      },
      "outputs": [],
      "source": [
        "class GroupedQueryAttention(nn.Module):\n",
        "    def __init__(self, embed_size, num_heads, num_query_groups):\n",
        "        super(GroupedQueryAttention, self).__init__()\n",
        "        assert embed_size % num_heads == 0, \"Embedding size must be divisible by number of heads\"\n",
        "        assert num_heads % num_query_groups == 0, \"Number of heads must be divisible by number of query groups\"\n",
        "\n",
        "        self.embed_size = embed_size\n",
        "        self.num_heads = num_heads\n",
        "        self.num_query_groups = num_query_groups\n",
        "        self.head_dim = embed_size // num_heads\n",
        "        self.group_dim = embed_size // num_query_groups\n",
        "\n",
        "        self.query_linear = nn.Linear(embed_size, embed_size)\n",
        "        self.key_linear = nn.Linear(embed_size, embed_size)\n",
        "        self.value_linear = nn.Linear(embed_size, embed_size)\n",
        "        self.fc_out = nn.Linear(embed_size, embed_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        batch_size, seq_length, embed_size = x.size()\n",
        "        #\n",
        "        Q = self.query_linear(x)  # (bsz, seq, emb)\n",
        "        K = self.key_linear(x)    # (bsz, seq, emb)\n",
        "        V = self.value_linear(x)  # (bsz, seq, emb)\n",
        "        #\n",
        "        Q = Q.view(batch_size, seq_length, self.num_query_groups, self.group_dim)\n",
        "        K = K.view(batch_size, seq_length, self.num_heads, self.head_dim)\n",
        "        V = V.view(batch_size, seq_length, self.num_heads, self.head_dim)\n",
        "        #\n",
        "        Q = Q.transpose(1, 2)  # (bsz, num_query_groups, seq, group_dim)\n",
        "        K = K.transpose(1, 2)  # (bsz, num_heads, seq, head_dim)\n",
        "        V = V.transpose(1, 2)  # (bsz, num_heads, seq, head_dim)\n",
        "\n",
        "        # G Q attn scores\n",
        "        group_size = self.num_heads // self.num_query_groups\n",
        "        attention_scores = []\n",
        "        for i in range(self.num_query_groups):\n",
        "            start_head = i * group_size\n",
        "            end_head = start_head + group_size\n",
        "            q_group = Q[:, i:i+1, :, :]  # (bsz, 1, seq, group_dim)\n",
        "            k_group = K[:, start_head:end_head, :, :]  # (bsz, group_size, seq, head_dim)\n",
        "            scores = torch.matmul(q_group, k_group.transpose(-2, -1)) / (self.group_dim ** 0.5)  # (bsz, 1, seq, seq)\n",
        "            attention_scores.append(scores)\n",
        "\n",
        "        attention_scores = torch.cat(attention_scores, dim=1)  # (bsz, num_heads, seq, seq)\n",
        "        attention_weights = F.softmax(attention_scores, dim=-1)  # (bsz, num_heads, seq, seq)\n",
        "        #\n",
        "        out = torch.matmul(attention_weights, V)  # (bsz, num_heads, seq, head_dim)\n",
        "        # concat\n",
        "        out = out.transpose(1, 2).contiguous().view(batch_size, seq_length, embed_size)  # (bsz, seq, emb)\n",
        "        #\n",
        "        out = self.fc_out(out)\n",
        "        return out, attention_weights\n",
        "\n",
        "# Usage\n",
        "batch_size, seq_length, embed_size, num_heads, num_query_groups = 2, 3, 4, 2, 1\n",
        "x = torch.randn(batch_size, seq_length, embed_size)\n",
        "grouped_query_attention_layer = GroupedQueryAttention(embed_size, num_heads, num_query_groups)\n",
        "out, attention_weights = grouped_query_attention_layer(x)\n",
        "\n",
        "print(\"Input:\", x)\n",
        "print(\"Output:\", out)\n",
        "print(\"Attention Weights:\", attention_weights)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vO3KnrbHW2j7"
      },
      "source": [
        "Ghost Attention will interact with the self-attention mechanism used for Transformer models, Ghost Attention is not itself a replacement for self-attention, rather a way to give the self-attention mechanism better data so it will remember instructions given early on over longer contexts.\n",
        "\n",
        "<figure>\n",
        "  <figcaption>Cross Attention</figcaption>\n",
        "  <img src=\"https://drive.google.com/uc?export=view&id=1hK5i\" alt=\"GAtt Image\" width=\"500\">\n",
        "</figure>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aXEFccObW3zk"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
